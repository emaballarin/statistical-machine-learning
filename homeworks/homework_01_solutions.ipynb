{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 01\n",
    "#### [An attempt to solution](https://github.com/emaballarin/statistical-machine-learning/blob/master/homeworks/homework_01_solutions.ipynb) by [Emanuele Ballarin](mailto:emanuele@ballarin.cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Let $Y$ be a *random variable* taking values in $\\{1,...,N\\}$ and suppose that any value is equally likely, i.e. that $P(Y=j)=\\frac{1}{N}$. \n",
    "\n",
    "Knowing that $\\sum_{j=1}^N j = \\frac{N(N+1)}{2}$, show that $\\mathbb{E}(Y)=\\frac{N+1}{2}$.\n",
    "\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "The *random variable* $Y$ is a discrete *r.v.* taking positive integer values in the closed interval $[1, N]$. As such, its *probability mass function* (discrete *p.d.f.*) is defined as:\n",
    "\n",
    "$$\n",
    "p(j) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{N} & \\text{for } j \\text{ integer, in } [1,N]\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To compute $\\mathbb{E}(Y)$, then, we apply the usual definition of *expectation value*.\n",
    "\n",
    "$\\mathbb{E}(Y) = \\sum_{j=-\\infty}^{+\\infty} j \\ p(j) = \\sum_{j=1}^{N} {{j}\\over{N}} = {{1}\\over{N}} \\sum_{j=1}^{N} j$.\n",
    "\n",
    "By exploiting given property, i.e. that $\\sum_{j=1}^N j = \\frac{N(N+1)}{2}$, we obtain\n",
    "\n",
    "$\\mathbb{E}(Y) = {{1}\\over{N}} \\sum_{j=1}^{N} j = \\frac{N(N+1)}{2 N} = \\frac{N+1}{2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Prove that $\\mathbb{E}(X)=\\mu$ for $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$.\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "In this case, $X$ is a continuous *random variable*. As such, the usual definition of *expectation value* holds: $\\mathbb{E}(X) = \\int_{-{\\infty}}^{+{\\infty}} x \\ p(x) dx $, which can easily be reduced to $\\mathbb{E}(X) = \\int_{Support(X)}^{} x \\ p(x) dx $. In such cases, $p(x)$ represents the *p.d.f* of the *r.v.* $X$ and $Support(X)$ its support.\n",
    "\n",
    "The latter passage is given for the sake of completeness, as in our *Normal-distribution* case the support of the *r.v.* is the full improper interval $]-{\\infty}, +{\\infty}[$.\n",
    "\n",
    "The *p.d.f.* of a Normal distribution, given mean $\\mu$ and variance $\\sigma^2$ is $\\mathcal{N}_{\\mu,\\sigma^2}(x) = \\frac 1{\\sqrt{2\\pi}\\sigma}\\exp \\left\\{-\\frac 12\\left( \\frac{x-\\mu}\\sigma \\right) ^2\\right\\}$ for x in $]-{\\infty}, +{\\infty}[$. In such cases, and in the following, the notation $exp\\{...\\}$ denotes the exponential function with natural base, and it is so rendered for the sake of inlining and readability.\n",
    "\n",
    "Therefore, we need to compute: $\\mathbb{E}(X) = \\int_{-{\\infty}}^{+{\\infty}} \\frac x{\\sqrt{2\\pi}\\sigma}\\exp \\left\\{-\\frac 12\\left( \\frac{x-\\mu}\\sigma \\right) ^2\\right\\} = \\frac 1{\\sqrt{2\\pi}\\sigma} \\int_{-{\\infty}}^{+{\\infty}} x \\exp \\left\\{-\\frac 12\\left( \\frac{x-\\mu}\\sigma \\right) ^2\\right\\}$, since $\\sigma$ does not depend on $x$.\n",
    "\n",
    "We try to make explicit the *Gaussian Integral* structure (which we suppose being known *w.r.t.* its definite integral over the whole real domain; *cfr. e.g. Jeffrey - Dai, \"Handbook of Mathematical Formulas and Integrals, 4th ed.\", Academic Press/Elsevier, 2008*), by operating the *change of variable*: $t = \\frac{x - \\mu}{\\sigma \\sqrt{2}}$\n",
    "\n",
    "As such, the integral becomes $\\mathbb{E}(X) = \\frac {\\sqrt 2 \\sigma} { \\sigma \\sqrt{2 \\pi} } \\int_{-\\infty}^{+\\infty} \\left({\\sqrt 2 \\sigma t + \\mu}\\right) \\exp \\left\\{{-t^2}\\right\\} dt = \\frac 1 {\\sqrt \\pi} \\left({\\sqrt 2 \\sigma \\int_{-\\infty}^{+\\infty} t \\exp \\left\\{{-t^2}\\right\\} dt + \\mu \\int_{-\\infty}^{+\\infty} \\exp \\left\\{{-t^2}\\right\\} dt}\\right)$\n",
    "\n",
    "Of the sum above, the former term contains the integral of an exponential, which can be directly *primitivated* and computed, the latter contains a *Gaussian Integral* that can be directly replaced by its value.\n",
    "\n",
    "$\\mathbb{E}(X) = \\frac 1 {\\sqrt \\pi} \\left({\\sqrt 2 \\sigma \\left[{-\\frac 1 2 \\exp \\left\\{{-t^2}\\right\\}}\\right]_{-\\infty}^{+\\infty} + \\mu \\sqrt \\pi}\\right)$\n",
    "\n",
    "The former term vanishes since leading to the $0 - 0 = 0$ form.\n",
    "\n",
    "$\\mathbb{E}(X) = \\frac{\\mu \\sqrt{\\pi}}{\\sqrt{\\pi}} = \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Let $X$ and $Y$ have discrete joint distribution\n",
    "$$\n",
    "p(x,y) = \\begin{cases}\\frac{1}{30} (x+y) & \\text{for } x = 0,1,2 \\text{ and } y=0,1,2,3\\\\0 & \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "\n",
    "Are $X$ and $Y$ independent?\n",
    "\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "In this case, the joint *p.d.f.* has a support which is easily exhaustible for the different values of *r.v.s* $X, Y$.  As such, we can easily compute the marginal *p.d.f.* for $X$ and $Y$ via direct summation on the marginal state-space of the other *r.v.*, thus leading to:\n",
    "$$\n",
    "p(x) = \\begin{cases}\\sum_{y=0}^{3} {\\frac{1}{30} (x+y)} & \\text{for } x = 0,1,2\\\\0 & \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p(y) = \\begin{cases}\\sum_{x=0}^{2} {\\frac{1}{30} (x+y)} & \\text{for } y = 0,1,2,3\\\\0 & \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "This leads to the following results:\n",
    "$$\n",
    "p(x) = \\begin{cases}\\frac{1}{15} (2x + 3) &\\text{for } x = 0,1,2\\\\0& \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p(y) = \\begin{cases}\\frac{1}{10} (y + 1) &\\text{for } y = 0,1,2,3\\\\0& \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "From these marginal *p.d.f.*s we can easily see that the independence condition is not satisfied, i.e. that $p(x,y) \\neq p(x)p(y)$ in at least one point of the domain.\n",
    "\n",
    "E.g. $p(0,0) = 0$ whereas $p_x(0)p_y(0) \\neq 0$.\n",
    "\n",
    "This proves that $X$ and $Y$ are not independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Let $X$ and $Y$ be two continuous random variables with joint probability density\n",
    "\n",
    "$$\n",
    "f(x,y) = \\begin{cases}24xy & \\text{for } 0<x<1,0<y<1,x+y<1\\\\0 & \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "\n",
    "Find:\n",
    "\n",
    "1. the marginal density of Y;\n",
    "2. the conditional density of $X$ given $Y=1/2$.\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "From the definition of *marginal probability density function*, we can obtain $f(y)$ by direct integration over the entire $X$-support left defined, i.e.\n",
    "$$\n",
    "f(y) = \\begin{cases}\\int_{0}^{1-y}24xy \\ dx & \\text{for } 0<y<1\\\\0 & \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "We can compute:\n",
    "\n",
    "$\\int_{0}^{1-y}24xy \\ dx = 24y \\ \\int_{0}^{1-y}x \\ dx = 24y \\ \\left[ {\\frac{x^2}{2}} \\right]_{0}^{1-y} = 12y \\ (1-y)^2$\n",
    "\n",
    "leading to:\n",
    "$$\n",
    "f(y) = \\begin{cases}12y \\ (1-y)^2 & \\text{for } 0<y<1\\\\0 & \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "We can also compute $f(x|y=0.5)$ by definition. In fact:\n",
    "\n",
    "$f(x|y) = \\frac{f(x,y)}{f(y)} = \\frac{24xy}{12y \\ (1-y)^2} = \\frac{2x}{(1-y)^2}$ on the appropriate support.\n",
    "\n",
    "By explicitly computing the above for $y=0.5$, we obtain finally:\n",
    "$$\n",
    "f(x|y=0.5) = \\begin{cases}8x & \\text{for } 0<x<0.5 \\ \\text{(since y=0.5, fixed)}\\\\0 & \\text{otherwise}\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Let the joint distribution of continuous *random variables* $X$ and $Y$ be \n",
    "\n",
    "$$f(X,Y) = \\frac{\\Gamma(\\alpha_1+\\alpha_2+\\alpha_3)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)\\Gamma(\\alpha_3)}x^{\\alpha_1-1}y^{\\alpha_2-1}(1-x-y)^{\\alpha_3-1}$$ for $0<x<1,0<y<1,x+y<1$ .\n",
    "\n",
    "Prove that the marginal distribution of $X$ is a $\\text{Beta}(\\alpha_1,\\alpha_2+\\alpha_3)$.\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "In the following, we assume as known w.r.t. their definite integrals the *(Euler) Beta integrals* (*cfr. e.g. Jeffrey - Dai, \"Handbook of Mathematical Formulas and Integrals, 4th ed.\", Academic Press/Elsevier, 2008*). This lightens follwing developments.\n",
    "\n",
    "For the same purpose, we define $\\Omega = \\frac{\\Gamma(\\alpha_1+\\alpha_2+\\alpha_3)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)\\Gamma(\\alpha_3)}$, being it constant w.r.t. $x$ and $y$.\n",
    "\n",
    "By definition of the *marginal p.d.f.* (directly restricted to its support w.r.t. integration over the domain) we can compute:\n",
    "$$\n",
    "f(X) = \\int_{0}^{1-x} \\Omega \\ {x^{{\\alpha_1}-1} \\ y^{{\\alpha_2}-1} \\ (1-x-y)^{{\\alpha_3}-1} dy} = \\\\ =\\Omega \\ {x^{{\\alpha_1}-1}} \\int_{0}^{1-x}{y^{{\\alpha_2}-1} \\ (1-x-y)^{{\\alpha_3}-1} dy}\n",
    "$$\n",
    "We can now perform the change of variable such that $y = (1-x)u$ in order to have to compute an integral in the $[0,1]$ interval w.r.t. $u$.\n",
    "\n",
    "This produces:\n",
    "$$\n",
    "f(X) = \\Omega \\ {x^{{\\alpha_1}-1}} \\int_{0}^{1} (1-x)^{{\\alpha_2}-1} \\ u^{{\\alpha_2}-1} \\ (1-x-(1-x)u)^{{\\alpha_3}-1} du = \\\\= \\Omega \\ {x^{{\\alpha_1}-1}} (1-x)^{\\alpha_2+\\alpha_3-1} \\int_0^1 u^{\\alpha_2-1} (1-u)^{\\alpha_3-1} du\n",
    "$$\n",
    "We can also replace the *Beta integral* $\\int_0^1 u^{\\alpha_2-1} (1-u)^{\\alpha_3-1} du$ with its value $\\frac{\\Gamma(\\alpha_2) \\Gamma(\\alpha_3)}{\\Gamma(\\alpha_2+\\alpha_3)}$.\n",
    "\n",
    "By re-expanding $\\Omega$ we obtain:\n",
    "$$\n",
    "f(X) = \\frac{\\Gamma(\\alpha_1+\\alpha_2+\\alpha_3)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)\\Gamma(\\alpha_3)} \\ {\\frac{\\Gamma(\\alpha_2) \\Gamma(\\alpha_3)}{\\Gamma(\\alpha_2+\\alpha_3)}} \\ {x^{{\\alpha_1}-1}} (1-x)^{\\alpha_2+\\alpha_3-1} = \\\\= \\frac{\\Gamma(\\alpha_1+\\alpha_2+\\alpha_3)}{\\Gamma(\\alpha_1){\\Gamma(\\alpha_2+\\alpha_3)}} \\ {x^{{\\alpha_1}-1}} (1-x)^{\\alpha_2+\\alpha_3-1}\n",
    "$$\n",
    "which is defined for $x$ in the $[0,1]$ domain, thus coinciding with the definition of the *p.d.f.* of a $\\text{Beta}(\\alpha_1,\\alpha_2+\\alpha_3)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
